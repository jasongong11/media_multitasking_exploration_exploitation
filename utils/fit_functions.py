import numpy as np

def loss_log_likelihood(x, earnings, choices):
    n_subj = int(len(earnings)/300)
    n_trial = 300
    
    sigma_d_hat = x[0]
    lambda_hat = x[1]
    theta_hat = x[2]
    
    betas = x[5:]
    
    P = np.zeros(choices.shape)
    sigma_o_hat = 0.04
    
    for i in range(n_subj):
        
        mu_pre_t_hat = np.ones(4) * x[3]
        sigma_pre_t_hat = np.ones(4) * x[4]
        
        choice = choices[300*i:300*i+300,:]
        earning = earnings[300*i:300*i+300]
        beta = betas[i]
        
        for j in range(n_trial):
            c_t = choice[j,:]
            r_t = earning[j]

            P_t = np.exp(mu_pre_t_hat/beta)/sum(np.exp(mu_pre_t_hat/beta))
            
            P[300*i+j,:] = P_t

            delta_t = r_t - mu_pre_t_hat

            k_t = sigma_pre_t_hat**2/(sigma_pre_t_hat**2 + sigma_o_hat**2) * c_t

            mu_post_t_hat = mu_pre_t_hat + k_t * delta_t

            sigma_post_t_hat = np.sqrt( (1-k_t) * sigma_pre_t_hat**2 )

            mu_pre_t_1_hat = lambda_hat * mu_post_t_hat + (1-lambda_hat) * theta_hat

            sigma_pre_t_1_hat = np.sqrt(lambda_hat**2 * sigma_post_t_hat**2 + sigma_d_hat**2)

            mu_pre_t_hat = mu_pre_t_1_hat

            sigma_pre_t_hat = sigma_pre_t_1_hat
    
    choice_P = np.sum(choices * P, axis=1)
    
    log_likelihood_kf = np.sum(np.log(choice_P))
    
    return -log_likelihood_kf


from multiprocessing import Pool

def loss_log_likelihood_softmax_ind(x, earning, choice):
    sigma_d_hat = x[0]
    lambda_hat = x[1]
    theta_hat = x[2]
    mu_pre_t_hat = np.ones(4) * x[3]
    sigma_pre_t_hat = np.ones(4) * x[4]
    beta = x[5]
    sigma_o_hat = 0.04
    
    P = np.zeros((300, 4))
    
    for i in range(300):
        c_t = choice[i,:]
        r_t = earning[i]
        
        P_t = np.exp(mu_pre_t_hat/beta)/sum(np.exp(mu_pre_t_hat/beta))
        
        P[i,:] = P_t
        
        delta_t = r_t - mu_pre_t_hat
        
        k_t = sigma_pre_t_hat**2/(sigma_pre_t_hat**2 + sigma_o_hat**2) * c_t
        
        mu_post_t_hat = mu_pre_t_hat + k_t * delta_t
        
        sigma_post_t_hat = np.sqrt( (1-k_t) * sigma_pre_t_hat**2 )
        
        mu_pre_t_1_hat = lambda_hat * mu_post_t_hat + (1-lambda_hat) * theta_hat
        
        sigma_pre_t_1_hat = np.sqrt(lambda_hat**2 * sigma_post_t_hat**2 + sigma_d_hat**2)
        
        mu_pre_t_hat = mu_pre_t_1_hat
        sigma_pre_t_hat = sigma_pre_t_1_hat
    
    choice_P = np.sum(choice * P, axis=1)
    log_likelihood_kf = np.sum(np.log(choice_P))
    
    return -log_likelihood_kf

def map_ind_softmax_func(ind_fit_data):
    earning = ind_fit_data[0]
    choice = ind_fit_data[1]
    x = ind_fit_data[2]
    
    log_likelihood_loss = loss_log_likelihood_softmax_ind(x, earning, choice)
    return log_likelihood_loss

def loss_log_likelihood_softmax_all(x, earnings, choices):
    n_subj = n_subj = int(len(earnings)/300)
    
    ind_fit_data = [(earnings[300*i:300*i+300], choices[300*i:300*i+300,:],
                     [x[0], x[1], x[2], x[3], x[4], x[5+i]]) for i in range(n_subj)]
    
    log_likelihoods = map(map_ind_softmax_func, ind_fit_data)
    
    sum_log_likelihoods = np.sum(list(log_likelihoods))
    
    return sum_log_likelihoods
    
    
def loss_log_likelihood_softmax_parallel(x, earnings, choices):
    n_subj = n_subj = int(len(earnings)/300)
    
    ind_fit_data = [(earnings[300*i:300*i+300], choices[300*i:300*i+300,:],
                     [x[0], x[1], x[2], x[3], x[4], x[5+i]]) for i in range(n_subj)]
    
    with Pool(processes=8) as P:
        log_likelihoods = P.map(map_ind_softmax_func, ind_fit_data)
    
    sum_log_likelihoods = np.sum(list(log_likelihoods))
    
    return sum_log_likelihoods  


def parameters(x0):
    return {"sigma_d_hat": x0[0], 
           "lambda_hat": x0[1],
           "theta_hat": x0[2],
           "mu_pre_t_hat": x0[3],
           "sigma_pre_t_hat": x0[4],
            "beta": x0[5:]
           }